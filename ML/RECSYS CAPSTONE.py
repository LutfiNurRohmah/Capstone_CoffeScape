# -*- coding: utf-8 -*-
"""RECSYS CAPSTONE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gDwwdFGnD4hAWy7yLAXYzKvnJRyIfaee
"""

import tensorflow as tf
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from tensorflow.keras.layers import Input
from sklearn.preprocessing import StandardScaler
import re

data = pd.read_csv('NEW_Dataset Capstone.xlsx - Rating Data (4).csv')

user_ids = data.columns[2:]
item_ids = data['Coffee IDs'].tolist()
print("User IDs:", user_ids)
print("Item IDs:", item_ids)

ratings_matrix = data.iloc[:, 2:].values
user_means = np.nanmean(ratings_matrix, axis=1, keepdims=True)
user_means[np.isnan(user_means)] = 0
for i in range(ratings_matrix.shape[0]):
    nan_indices = np.isnan(ratings_matrix[i, :])
    ratings_matrix[i, nan_indices] = user_means[i]
scaler = StandardScaler()
ratings_matrix_normalized = scaler.fit_transform(ratings_matrix)
X_train, X_test = train_test_split(ratings_matrix, test_size=0.2, random_state=42)

num_users = len(user_ids)
num_items = len(item_ids)
embedding_size = 50

user_input = Input(shape=(1,), name='user_input')
user_embedding = tf.keras.layers.Embedding(input_dim=num_users, output_dim=embedding_size, input_length=1)(user_input)
user_embedding = tf.keras.layers.Flatten()(user_embedding)
item_input = Input(shape=(1,), name='item_input')
item_embedding = tf.keras.layers.Embedding(input_dim=num_items, output_dim=embedding_size, input_length=1)(item_input)
item_embedding = tf.keras.layers.Flatten()(item_embedding)

concatenate= tf.keras.layers.Concatenate()([user_embedding, item_embedding])
concatenated = tf.keras.layers.Dropout(0.2)(concatenate)

dense_layer_1 = tf.keras.layers.Dense(64, activation='relu')(concatenated)
dense_layer_1= tf.keras.layers.Dropout(0.2)(dense_layer_1)
dense_layer_2= tf.keras.layers.Dense(32, activation='relu')(dense_layer_1)
output = tf.keras.layers.Dense(1, activation='relu')(dense_layer_2)

model= tf.keras.models.Model(inputs=[user_input, item_input], outputs=output)

model.compile(optimizer='adam', loss='mean_squared_error')

model.fit(x=[X_train[:, 0], X_train[:, 1]], y=X_train[:, 2], epochs=1000, batch_size=32, validation_split=0.2)

user_id_to_predict = 1
item_id_to_predict = 3
prediction = model.predict([np.array([user_id_to_predict]), np.array([item_id_to_predict])])
print(f'Predicted Rating: {prediction}')

user_ids_int = [int(re.search(r'\d+', user_id).group()) for user_id in user_ids]
new_user_id = max(user_ids_int) + 1
new_user_ratings = []
num_items = len(item_ids)
for item_id in range(1, num_items + 1):
    rating = input(f"Enter the rating for item {item_id} (or enter 0 if not rated): ")
    new_user_ratings.append(int(rating))
new_user_data = np.column_stack(([new_user_id] * num_items, np.arange(1, num_items + 1), new_user_ratings))
data = np.vstack([data, new_user_data])

user_id_for_recommendation = new_user_id
all_item_ids = np.arange(1, num_items + 1)

items_not_rated = np.setdiff1d(all_item_ids, X_train[X_train[:, 0] == user_id_for_recommendation][:, 1])

user_predictions = model.predict([np.full_like(items_not_rated, user_id_for_recommendation), items_not_rated])

recommendations = list(zip(items_not_rated, user_predictions.flatten()))

recommendations.sort(key=lambda x: x[1], reverse=True)

top_n = 5
print(f'Top {top_n} Recommendations for New User {user_id_for_recommendation}:')
for i, (item_id, predicted_rating) in enumerate(recommendations[:top_n], 1):
    print(f'{i}. Item {item_id}, Predicted Rating: {predicted_rating}')